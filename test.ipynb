{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import mediapy\n",
    "import pickle\n",
    "import torch\n",
    "from scipy.io import savemat\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from auxiliar.tracking_viz import generate_frame_image_cv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from src.ortho_factorization import marques_factorization\n",
    "import rerun as rr\n",
    "from src.mat_compl import alternating_matrix_completion\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from auxiliar.read_video import read_video_or_images\n",
    "from src.tapnext_infer import init_tapnext\n",
    "\n",
    "tapnext_model_video_size = (256, 256)\n",
    "device = \"cuda:0\"\n",
    "\n",
    "#path = \"../mast3r/images_in/casa_bea_4_photos\"\n",
    "path = \"../mast3r/images_in/casa_bea_4_photos\"\n",
    "#video_resized, video = read_video_or_images(\"../mast3r/images_in/bordalo.mp4\",model_video_size, device)\n",
    "\n",
    "video_square, _ = read_video_or_images(path)\n",
    "print(video_square.shape)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the query points array in the original video size\n",
    "step = 70\n",
    "ys, xs = np.meshgrid(np.linspace(8, video_square[0].shape[0]-step, step), np.linspace(8, video_square[0].shape[1]-step, step))\n",
    "query_points_initial = torch.tensor(\n",
    "    np.stack(\n",
    "    [np.zeros(len(xs.flatten())), xs.flatten(), ys.flatten()], axis=1\n",
    ")[None],dtype=torch.float32).to(device)\n",
    "\n",
    "#creata tensors for the video information\n",
    "frame_cut = max(20, len(video_square))\n",
    "#frame_cut = 106\n",
    "frame_skip = 1\n",
    "video_tensor = torch.cat([(frame).unsqueeze(0) for frame in video_square]).unsqueeze(0)[:,0:frame_cut:frame_skip,...] # 1, time, H, W, 3\n",
    "video_tensor = torch.clip(video_tensor, -1, 1)\n",
    "num_frames = video_tensor.shape[1]\n",
    "\n",
    "print(\"frames:\", num_frames)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tapnext_infer import run_tapnext, init_alltracker\n",
    "from src.new_queries import add_new_tracks\n",
    "from auxiliar.read_video import resize_to_max_side\n",
    "import utils.saveload\n",
    "import utils.basic\n",
    "import utils.improc\n",
    "import torch.nn.functional as F\n",
    "\n",
    "mode = \"tapnext\"  # choose: \"alltracker\", \"tapnext\", \"cotracker\"\n",
    "\n",
    "match mode:\n",
    "    # ==============================================================\n",
    "    case \"alltracker\":\n",
    "\n",
    "        model = init_alltracker(device)\n",
    "\n",
    "        rgbs = resize_to_max_side((video_tensor.permute(0,1,4,2,3) + 1) / 2 * 255)\n",
    "        B, T, C, H, W = rgbs.shape\n",
    "        \n",
    "        flows_e, visconf_maps_e, _, _ = model.forward_sliding(\n",
    "            rgbs[:, 0:], iters=4, sw=None, is_training=False\n",
    "        )\n",
    "\n",
    "        # grid in pixel coords\n",
    "        grid_xy = utils.basic.gridcloud2d(1, H, W, norm=False, device=\"cpu\").float()  # [1,H*W,2]\n",
    "        grid_xy = grid_xy.permute(0, 2, 1).reshape(1, 1, 2, H, W)  # [1,1,2,H,W]\n",
    "\n",
    "        # flows_e: [1, T, 2, H, W]\n",
    "        # grid_xy: [1, 1, 2, H, W]\n",
    "        traj_maps_e = flows_e + grid_xy  # [1, T, 2, H, W]\n",
    "\n",
    "        # → [1, T, H, W, 2]\n",
    "        traj_maps_e = traj_maps_e.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "        # get forward visibility confidence (channel 1)\n",
    "        vis_forward = visconf_maps_e[:, :, 1, :, :]  # [1, T, H, W]\n",
    "\n",
    "        # apply a threshold (e.g. > 0.5)\n",
    "        visible_mask = vis_forward > 0.75             # [1, T, H, W]\n",
    "        visible_mask = visible_mask.unsqueeze(-1)    # [1, T, H, W, 1]\n",
    "        visible_mask = visible_mask.expand_as(traj_maps_e)  # [1, T, H, W, 2]\n",
    "\n",
    "        traj_maps_e[~visible_mask] = float('nan')\n",
    "\n",
    "\n",
    "        # get original video shape\n",
    "        H_orig, W_orig = video_tensor.shape[2:4]\n",
    "        scale_x = W_orig / W  # width ratio\n",
    "        scale_y = H_orig / H  # height ratio\n",
    "        scale = torch.tensor([scale_x, scale_y], device=traj_maps_e.device)\n",
    "\n",
    "        # apply before flattening\n",
    "        traj_maps_e = traj_maps_e * scale\n",
    "\n",
    "        # flatten to [1, T, H*W, 2]\n",
    "        output_tensor = traj_maps_e.reshape(1, traj_maps_e.shape[1], H * W, 2)  # [1, T, N, 2]\n",
    "\n",
    "    # ==============================================================\n",
    "    case \"tapnext\":\n",
    "        \n",
    "        tapnext = init_tapnext(device, \"/home/manuelf/storage/bootstapnext_ckpt.npz\")\n",
    "        print(\"Model initialized\")\n",
    "\n",
    "        video_permuted = video_tensor.clone().permute(0, 4, 1, 2, 3) \n",
    "        target_frames = video_permuted.shape[2] # Keep frames as 17\n",
    "        target_height = 256\n",
    "        target_width = 256\n",
    "        resized_video = F.interpolate(\n",
    "            video_permuted, \n",
    "            size=(target_frames, target_height, target_width), \n",
    "            mode='trilinear',  # Standard for 5D tensors\n",
    "            align_corners=False\n",
    "        )\n",
    "        final_video = resized_video.permute(0, 2, 3, 4, 1)\n",
    "\n",
    "        query_points_tapnext = query_points_initial.clone()\n",
    "        query_points_tapnext[0, :, 1:3] /= torch.tensor([video_tensor.shape[3] / final_video.shape[3], video_tensor.shape[2] / final_video.shape[2]]).to(device) # 1, N, 3(frames,x,y)\n",
    "\n",
    "        track_histories = run_tapnext(\n",
    "            final_video.to(device),  # send the video resized to the model size\n",
    "            query_points_tapnext,\n",
    "            tapnext,\n",
    "            device=device,\n",
    "            new_tracks_flag=False,  # or None\n",
    "        )\n",
    "        output = {}\n",
    "\n",
    "        for track_id, trajectory in tqdm(track_histories.items()):  # per feature\n",
    "            coords = []\n",
    "            for t in range(num_frames):  # per frame\n",
    "                step = next((pos for (frame, pos) in trajectory if frame == t), None)\n",
    "                coords.append(step if step is not None else torch.tensor([float('nan'), float('nan')]))\n",
    "            output[track_id] = torch.stack(coords)  # shape: [num_frames, 2]\n",
    "\n",
    "        output_list = [trajectory.unsqueeze(0) for _, trajectory in output.items()]\n",
    "        output_tensor = torch.cat(output_list, dim=0).unsqueeze(0).permute(0, 2, 1, 3)  # [1, num_frames, num_feats, 2]\n",
    "\n",
    "        # Flip x/y and scale back to original video size\n",
    "        output_tensor = output_tensor[:, :, :, [1, 0]]\n",
    "        output_tensor *= torch.tensor([\n",
    "            video_tensor.shape[3] / final_video.shape[3],\n",
    "            video_tensor.shape[2] / final_video.shape[2]\n",
    "        ])\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # ==============================================================\n",
    "    case \"cotracker\":\n",
    "        cotracker = torch.hub.load(\"facebookresearch/co-tracker\", \"cotracker3_offline\").to(device)\n",
    "\n",
    "        pred_tracks, pred_visibility = cotracker(\n",
    "            video_tensor.squeeze().permute(0, 3, 1, 2).unsqueeze(0).float().to(device),\n",
    "            grid_size=step,\n",
    "            thr=0.9,\n",
    "        )  # B T N 2,  B T N 1\n",
    "\n",
    "        pred_tracks = pred_tracks.cpu()\n",
    "        pred_visibility = pred_visibility.cpu()\n",
    "\n",
    "        # Ensure shape [1, T, N, 1]\n",
    "        if pred_visibility.ndim == 3:\n",
    "            pred_visibility = pred_visibility.unsqueeze(-1)\n",
    "\n",
    "        # Visibility mask\n",
    "        visible_mask = pred_visibility.expand_as(pred_tracks)\n",
    "        pred_tracks_masked = pred_tracks.clone()\n",
    "        pred_tracks_masked[~visible_mask] = float('nan')\n",
    "\n",
    "        # Optional flip if you want consistency\n",
    "        # pred_tracks_masked = pred_tracks_masked[:, :, :, [1, 0]]\n",
    "\n",
    "        output_tensor = pred_tracks_masked  # [1, num_frames, num_feats, 2]\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # ==============================================================\n",
    "    case _:\n",
    "        raise ValueError(f\"Unknown tracking mode: {mode}\")\n",
    "\n",
    "# Unified final output: output_tensor → shape [1, num_frames, num_feats, 2]\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frame_cut_out = output_tensor.shape[1]\n",
    "\n",
    "# Calculate the number of visible frames for each track\n",
    "visible_counts = torch.sum(~torch.isnan(output_tensor[0, :frame_cut_out, :, 0]), dim=0) #shape: #feats\n",
    "\n",
    "# Filter tracks that are visible for at least 20 frames\n",
    "visible_tracks_mask = visible_counts >= 1\n",
    "output_tensor_filtered = output_tensor[:, :frame_cut_out, visible_tracks_mask, :]\n",
    "output_tensor_filtered.shape\n",
    "\n",
    "video_tensor_original = video_tensor.to(\"cpu\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_frame_image_cv(video_tensor_original.cpu(), output_tensor[:,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from src.interpolations import get_rgb_at_coords_torch\n",
    "\n",
    "# output_tensor_filtered: [1, num_frames, num_feats, 2]\n",
    "num_frames = output_tensor_filtered.shape[1]\n",
    "num_feats = output_tensor_filtered.shape[2]\n",
    "\n",
    "# Build observation matrix: [feats*2, frames]\n",
    "obs_mat_full = torch.full((num_frames * 2, num_feats), float(\"nan\"), device=output_tensor_filtered.device)\n",
    "\n",
    "for frame in range(num_frames):  # per frame\n",
    "    obs_mat_full[frame*2, :] = output_tensor_filtered[0, frame, :, 0]  # x\n",
    "    obs_mat_full[frame*2+1, :] = output_tensor_filtered[0, frame, :, 1]  # y\n",
    "\n",
    "# remove columns (features) that have any NaN values\n",
    "valid_columns_mask_no_nan = ~torch.isnan(obs_mat_full).any(dim=0)\n",
    "obs_mat = obs_mat_full[:, valid_columns_mask_no_nan]\n",
    "\n",
    "# subsample columns (features) by a factor of 50\n",
    "#obs_mat = obs_mat[:, ::10]\n",
    "\n",
    "# remove columns with more than ~50% NaN values\n",
    "nan_counts = torch.isnan(obs_mat_full).sum(dim=0)\n",
    "valid_columns_mask_half = nan_counts < (obs_mat_full.shape[0] / 1.3)\n",
    "obs_mat_half_missed = obs_mat_full[:, valid_columns_mask_half]\n",
    "\n",
    "# percentage of missing values\n",
    "missing_percentage = (\n",
    "    torch.isnan(obs_mat_half_missed).sum().float()\n",
    "    / (obs_mat_half_missed.numel())\n",
    "    * 100\n",
    ").item()\n",
    "\n",
    "print(f\"Percentage of missing values: {missing_percentage:.2f}%\")\n",
    "\n",
    "# plot with torch → numpy just for visualization\n",
    "#plt.figure(figsize=(12, 6))\n",
    "#plt.imshow((obs_mat_half_missed), aspect=\"auto\", cmap=\"gray\" )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# obs_mat: [2*frames, feats]\n",
    "# video: [frames, H, W, C] in [-1, 1] (assumed)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))  # Create a 2x3 grid\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Predefined features (convert to torch if you want consistency)\n",
    "features = torch.tensor([3, 9, 315, 227, 266, 207, 107])\n",
    "features = torch.tensor([3, 9])\n",
    "rand_feats = torch.randint(0, obs_mat.shape[1], (20,))\n",
    "\n",
    "features = torch.cat([features, rand_feats])\n",
    "print(\"Selected features:\", features.tolist())\n",
    "\n",
    "# Loop through frames\n",
    "cnt = 0\n",
    "for i in torch.linspace(0, video_tensor.cpu().shape[1]-1, 6).to(torch.int):  # num_frames\n",
    "    # Convert frame for plotting\n",
    "    frame_img = ((video_tensor[0,i].cpu() + 1) / 2.0)\n",
    "    axes[cnt].imshow(frame_img)\n",
    "\n",
    "    axes[cnt].scatter(obs_mat[i*2, features], obs_mat[i*2+1, features], s=6, c=\"red\")\n",
    "    axes[cnt].set_title(f\"Frame {i+1}\")\n",
    "    axes[cnt].axis(\"off\")\n",
    "    cnt += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ortho_factorization import costeira_marques\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "obs_completed, _ = alternating_matrix_completion(obs_mat, mode=0, max_iters=200, rank=3)\n",
    "# completion + factorization\n",
    "motion, shape, t,_ = costeira_marques(obs_mat) # rank 3\n",
    "reconstructed_obs = motion @ shape + t.repeat(1, shape.shape[1])\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot the completed observation matrix\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(obs_completed / np.nanmax(obs_completed), aspect='auto', cmap='gray')\n",
    "plt.title(\"Rank constrained Observation Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot the original observation matrix\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(obs_mat / np.nanmax(obs_mat), aspect='auto', cmap='gray')\n",
    "plt.title(\"Original Observation Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot the singular values\n",
    "plt.subplot(1, 3, 3)\n",
    "singular_values_rank = torch.linalg.svd(obs_completed, full_matrices=False).S\n",
    "plt.plot(singular_values_rank.cpu().numpy(), \"-x\", linewidth=2)\n",
    "\n",
    "singular_values_original = torch.linalg.svd(obs_mat, full_matrices=False).S\n",
    "plt.plot(singular_values_original.cpu().numpy(), \"-o\")\n",
    "plt.legend([\"rank_constrained\", \"Original\"])\n",
    "plt.title(\"Singular Values\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "#plot the singular values of the reconstructed\n",
    "plt.figure(figsize=(10, 6))\n",
    "singular_values_reconstructed = torch.linalg.svd(reconstructed_obs, full_matrices=False).S\n",
    "plt.plot(singular_values_reconstructed.cpu().numpy(), \"-o\", linewidth=2)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Singular Values of Reconstructed Observation Matrix\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliar.depth_tensor_viz import k3d_3d_plot\n",
    "k3d_3d_plot(shape.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "sys.path.append('../boxes/vggt/protos')\n",
    "import pipeline_pb2 as vggt_pb2\n",
    "import pipeline_pb2_grpc as vggt_pb2_grpc\n",
    "from aux import wrap_value, unwrap_value\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "image_byte_list = []\n",
    "# List all files in the directory\n",
    "if 0:\n",
    "    \n",
    "    files = [os.path.join(path, file) for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n",
    "\n",
    "    for file in sorted(files):\n",
    "        # Open the file in binary read mode ('rb') and read its entire content\n",
    "        with open(file, 'rb') as f:\n",
    "            image_bytes = f.read()\n",
    "            image_byte_list.append(image_bytes)\n",
    "            print(f\"Read {file}: {len(image_bytes) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "else:\n",
    "\n",
    "    # Convert video frames (tensor) to a list of byte arrays\n",
    "    for frame in video_tensor[0]:  # Assuming video_tensor_original is [1, time, H, W, C]\n",
    "        # Convert tensor to numpy array and scale to [0, 255]\n",
    "        frame_np = (frame.numpy() * 255).astype('uint8')\n",
    "        # Create an Image object\n",
    "        img = Image.fromarray(frame_np)\n",
    "        # Save the image to a bytes buffer\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"JPEG\")\n",
    "        # Append the bytes to the list\n",
    "        image_byte_list.append(buffer.getvalue())\n",
    "\n",
    "config_json = {\n",
    "        \"command\": \"3d_infer\",\n",
    "        \"parameters\": {\n",
    "            \"device\": \"cuda:0\", # TODO: implement this\n",
    "            \"conf_vis\": 50.0\n",
    "        }\n",
    "}\n",
    "\n",
    "request = vggt_pb2.Envelope(data={\"images\":wrap_value(image_byte_list)},\n",
    "                            config_json = json.dumps(config_json))\n",
    "channel_opt = [('grpc.max_send_message_length', -1), ('grpc.max_receive_message_length', -1)]\n",
    "channel=grpc.insecure_channel(\"localhost:8061\",options=channel_opt)\n",
    "estimator_stub = vggt_pb2_grpc.PipelineServiceStub(channel)\n",
    "response = estimator_stub.Process(request)\n",
    "channel.close()\n",
    "\n",
    "#write the glb as a file\n",
    "glb_file = unwrap_value(response.data[\"glb_file\"])\n",
    "with open(\"output.glb\", \"wb\") as f:\n",
    "    f.write(glb_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliar.depth_tensor_viz import k3d_3d_plot, plot_depth_tensor_grid\n",
    "wrld_pts = np.load(io.BytesIO(unwrap_value(response.data[\"world_points\"]))).squeeze().reshape(-1,3)\n",
    "wrld_conf = np.load(io.BytesIO(unwrap_value(response.data[\"world_points_conf\"]))).squeeze().reshape(-1,1)\n",
    "                    \n",
    "k3d_3d_plot(wrld_pts[wrld_conf.flatten() > 1, :][::100].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_depth_tensor_grid(torch.tensor(np.load(io.BytesIO(unwrap_value(response.data[\"depth\"])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from src.projective_factorization import build_depth_weighted_matrix, normalize_measurement_matrix\n",
    "\n",
    "vggt_depth_tensor = torch.tensor(np.load(io.BytesIO(unwrap_value(response.data[\"depth\"]))))  # 1, time, H, W, 1\n",
    "vggt_conf_tensor = torch.tensor(np.load(io.BytesIO(unwrap_value(response.data[\"depth_conf\"]))))  # 1, time, H, W, 1\n",
    "instrisics = torch.tensor(np.load(io.BytesIO(unwrap_value(response.data[\"intrinsic\"]))))\n",
    "extrinsics = torch.tensor(np.load(io.BytesIO(unwrap_value(response.data[\"extrinsic\"]))))\n",
    "wrld_pts = torch.tensor(np.load(io.BytesIO(unwrap_value(response.data[\"world_points\"])))) # 1, #frame, H, W, 3\n",
    "\n",
    "# need to resize the observation matrix to the size of the depth tensor\n",
    "W_vggt = obs_mat.clone()\n",
    "W_vggt[0::2,:] *=  (vggt_depth_tensor.shape[2] / video_tensor.shape[2])\n",
    "W_vggt[1::2,:] *=  (vggt_depth_tensor.shape[3] / video_tensor.shape[3])\n",
    "\n",
    "# average all intrinsic matrices\n",
    "K_vggt = torch.mean(instrisics, dim=0) # TODO: this needs to be resized due to the tracks\n",
    "#K = torch.eye(3)\n",
    "\n",
    "frames_to_use = vggt_depth_tensor.shape[1]\n",
    "W_depth_vggt, tracks_lambda_vggt = build_depth_weighted_matrix(W_vggt[0:frames_to_use*2,:], vggt_depth_tensor.squeeze()[0:frames_to_use,:],\n",
    "                                            Ks=instrisics)\n",
    "\n",
    "pattern = torch.tensor([1, 1, 0], dtype=torch.bool)\n",
    "mask = pattern.repeat((W_depth_vggt.shape[0] + len(pattern) - 1) // len(pattern))[:W_depth_vggt.shape[0]]\n",
    "W_vggt_nohomg = W_depth_vggt[mask, ...]\n",
    "\n",
    "# Normalize the measurement matrix with isotropic scaling sqrt(3)\n",
    "Wn, T_list = normalize_measurement_matrix(W_depth_vggt)                     # Wn: [3F, P]\n",
    "\n",
    "U, S, Vh = torch.linalg.svd(W_depth_vggt, full_matrices=False)\n",
    "r = 4\n",
    "Ur = U[:, :r]\n",
    "Sr = torch.diag(S[:r])\n",
    "Vr = Vh[:r, :]\n",
    "\n",
    "Ssqrt = torch.linalg.cholesky(Sr)  \n",
    "Motion = Ur @ Ssqrt                     \n",
    "Sshape = Ssqrt @ Vr                # (4,  P) \n",
    "W_hat = Motion @ Sshape\n",
    "\n",
    "#X3d = Sshape[:3, :] / Sshape[3:4, :]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the normalized measurement matrix\n",
    "im1 = axes[0].imshow(W_depth_vggt, aspect='auto', cmap='gray', interpolation='none')\n",
    "axes[0].set_title(\"Normalized Measurement Matrix\")\n",
    "fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Plot the singular values in log scale\n",
    "axes[1].plot(torch.linalg.svd(W_depth_vggt)[1], \"-x\")\n",
    "axes[1].set_xlabel(\"Index\")\n",
    "axes[1].set_title(\"Log10 Singular Values\")\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization of the W_hat with the no homogenous coordinates and the lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.projective_factorization import metric_upgrade_daq\n",
    "U, S, V = torch.linalg.svd(W_depth_vggt)\n",
    "\n",
    "r = 4\n",
    "Ur = U[:, :r]\n",
    "Sr = torch.diag(S[:r])\n",
    "Vr = Vh[:r, :]\n",
    "\n",
    "Ssqrt = torch.linalg.cholesky(Sr)  \n",
    "Motion = Ur @ Ssqrt                     \n",
    "Shape = Ssqrt @ Vr\n",
    "\n",
    "H, P_e, R_list, t_list = metric_upgrade_daq(Motion, [K_vggt for _ in range(W_depth_vggt.shape[0] // 3)])\n",
    "\n",
    "X_e = H @ Shape       # Shape = 4×P from your projective factorization\n",
    "X_e = X_e / X_e[3]\n",
    "theta = lambda R :torch.arccos(((torch.trace(R) - 1) / 2).clamp(-1, 1))\n",
    "R_norm = [theta(R) for R in R_list]\n",
    "R_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another depth estimator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/home/manuelf/UniDepth/\")\n",
    "from unidepth.models import UniDepthV2\n",
    "\n",
    "device = \"cpu\"\n",
    "model = UniDepthV2.from_pretrained(\"lpiccinelli/unidepth-v2-vitl14\").to(device) # or \"lpiccinelli/unidepth-v1-cnvnxtl\" for the ConvNext backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "uni_conf = []\n",
    "uni_focal = []\n",
    "\n",
    "for frame in tqdm(range(video_tensor.shape[1])):\n",
    "    pred = model.infer(video_tensor[0,frame].permute(2,0,1).to(device))\n",
    "    #rnd = torch.rand(1).to(device) * 100\n",
    "    rnd = 1\n",
    "    #print(rnd)\n",
    "    depths.append(pred[\"depth\"] * rnd)\n",
    "    uni_conf.append(1 / pred[\"confidence\"])\n",
    "    uni_focal.append(pred[\"intrinsics\"])\n",
    "\n",
    "uni_depth_tensor = torch.stack(depths).unsqueeze(-1).cpu().squeeze()  # (num_frames, H, W, 1)\n",
    "uni_conf_tensor = torch.stack(uni_conf).unsqueeze(-1).cpu().squeeze()  # (num_frames, H, W, 1)\n",
    "uni_focal_tensor = torch.stack(uni_focal).cpu().squeeze()  # (num_frames, 3, 3)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.projective_factorization import build_depth_weighted_matrix, normalize_measurement_matrix\n",
    "\n",
    "# make the tracks fit into the new depth size\n",
    "W_uni = obs_mat.clone()\n",
    "W_uni[0::2,:] = W_uni[0::2] * (uni_depth_tensor.shape[1] / video_tensor.shape[2])\n",
    "W_uni[1::2,:] = W_uni[1::2] * (uni_depth_tensor.shape[2] / video_tensor.shape[3])\n",
    "\n",
    "#K_uni_mean = torch.mean(uni_focal_tensor, dim=0)\n",
    "#K_uni_mean = torch.eye(3)\n",
    "\n",
    "frames_to_use = uni_depth_tensor.shape[0]\n",
    "W_depth_uni, tracks_lambda_unidepth = build_depth_weighted_matrix(W_uni[0:frames_to_use*2,:], uni_depth_tensor[0:frames_to_use,:],\n",
    "                                            uni_focal_tensor) \n",
    "\n",
    "pattern = torch.tensor([1, 1, 0], dtype=torch.bool)\n",
    "mask = pattern.repeat((W_depth_uni.shape[0] + len(pattern) - 1) // len(pattern))[:W_depth_uni.shape[0]]\n",
    "W_uni_nohomg = W_depth_uni[mask, ...]\n",
    "\n",
    "Wn, T_list = normalize_measurement_matrix(W_depth_uni)                     # Wn: [3F, P]\n",
    "\n",
    "U, S, Vh = torch.linalg.svd(Wn, full_matrices=False)\n",
    "r = 4\n",
    "Ur = U[:, :r]\n",
    "Sr = torch.diag(S[:r])\n",
    "Vr = Vh[:r, :]\n",
    "\n",
    "Ssqrt = torch.linalg.cholesky(Sr)  \n",
    "Motion = Ur @ Ssqrt                     \n",
    "Sshape = Ssqrt @ Vr                # (4,  P) \n",
    "W_hat = Motion @ Sshape\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the normalized measurement matrix\n",
    "im1 = axes[0].imshow(W_uni_nohomg, aspect='auto', cmap='gray', interpolation='none')\n",
    "axes[0].set_title(\"Normalized Measurement Matrix\")\n",
    "fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Plot the singular values in log scale\n",
    "axes[1].plot(torch.linalg.svd(W_uni_nohomg)[1], \"-x\")\n",
    "axes[1].set_xlabel(\"Index\")\n",
    "#axes[1].set_title(\"Log10 Singular Values\")\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliar.depth_tensor_viz import k3d_3d_plot\n",
    "k3d_3d_plot(marques_factorization(W_uni_nohomg)[2]) # tomasi\n",
    "#k3d_3d_plot(marques_factorization(W_depth_uni[:-1,:])[2]) # tomasi\n",
    "#plt.imshow(costeira_marques(W_uni_nohomg)[1], aspect='auto', cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ortho_factorization import calibrate_orthographic, random_affine_camera\n",
    "\n",
    "\n",
    "F, P = 6, 100\n",
    "shape = torch.rand(4, P)\n",
    "\n",
    "motion = torch.stack([random_affine_camera() for _ in range(F)]).reshape(-1, 4)  # [2F, 4]\n",
    "rand_lambda = random_tensor = (1.2 - 0.5) * torch.rand(F,P) + 0.5\n",
    "\n",
    "rand_W = (motion @ shape) / rand_lambda.repeat_interleave(2, dim=0)\n",
    "\n",
    "rand_lambda_hat = rand_lambda.clone()\n",
    "\n",
    "# add a random offset to each line of rand_lambda_hat\n",
    "offsets_list = [torch.rand(1).item() * 5.0 for _ in range(F)]\n",
    "for f in range(F):\n",
    "    rand_lambda_hat[f, :] += offsets_list[f]\n",
    "\n",
    "sigma = 0.05  # 5% relative error\n",
    "noise = torch.randn_like(rand_lambda) * sigma\n",
    "noisy_lambda = rand_lambda_hat * torch.exp(noise)\n",
    "\n",
    "scales, offsets, W, M, first_W, z = calibrate_orthographic(\n",
    "    rand_W, noisy_lambda, K=torch.eye(3),\n",
    "    iters=100, rank=4, tol=1e-6\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(torch.linalg.svdvals(first_W)[:7], \"-x\", label=\"before\")\n",
    "plt.plot(torch.linalg.svdvals(W)[:7], \"-x\", label=\"after\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10,8), sharex=True)\n",
    "\n",
    "num_iters, F = scales.shape\n",
    "iters = torch.arange(num_iters)\n",
    "\n",
    "# --- subplot 1: scales ---\n",
    "for f in range(F):\n",
    "    axs[0].plot(iters, scales[:, f].cpu(), \"-o\", label=f\"frame {f}\")\n",
    "\n",
    "axs[0].set_ylabel(\"Scale d[f]\")\n",
    "axs[0].set_title(\"Per-frame scale convergence\")\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(ncol=3, fontsize=8)\n",
    "\n",
    "# --- subplot 2: offsets ---\n",
    "for f in range(F):\n",
    "    axs[1].plot(iters, offsets[:, f].cpu()/scales[:, f].cpu(), \"-o\", label=f\"frame {f}\")\n",
    "    axs[1].hlines(-offsets_list[f], -1, iters[-1])\n",
    "\n",
    "axs[1].set_xlabel(\"Iteration\")\n",
    "axs[1].set_ylabel(\"Offset s[f]\")\n",
    "axs[1].set_title(\"Per-frame offset convergence\")\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_to_use = torch.arange(uni_depth_tensor.shape[0])\n",
    "indices = torch.stack((frames_to_use * 2, frames_to_use * 2 + 1), dim=1).flatten()\n",
    "\n",
    "scales, offsets, W, M, first_W, z = calibrate_orthographic(\n",
    "    W_uni[indices], # Use the interleaved indices\n",
    "    tracks_lambda_unidepth[frames_to_use,:], \n",
    "    K=torch.eye(3),\n",
    "    iters=100, rank=4, tol=1e-8\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(torch.linalg.svdvals(first_W)[:7], \"-x\", label=\"before\")\n",
    "plt.plot(torch.linalg.svdvals(W)[:7], \"-x\", label=\"after\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10,8), sharex=True)\n",
    "\n",
    "num_iters, F = scales.shape\n",
    "iters = torch.arange(num_iters)\n",
    "\n",
    "# --- subplot 1: scales ---\n",
    "for f in range(F):\n",
    "    axs[0].plot(iters, scales[:, f].cpu(), \"-o\", label=f\"frame {f}\")\n",
    "\n",
    "axs[0].set_ylabel(\"Scale d[f]\")\n",
    "axs[0].set_title(\"Per-frame scale convergence\")\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(ncol=3, fontsize=8)\n",
    "\n",
    "# --- subplot 2: offsets ---\n",
    "for f in range(F):\n",
    "    axs[1].plot(iters, offsets[:, f].cpu() , \"-o\", label=f\"frame {f}\")\n",
    "    #axs[1].hlines(-offsets_list[f], -1, iters[-1])\n",
    "\n",
    "axs[1].set_xlabel(\"Iteration\")\n",
    "axs[1].set_ylabel(\"Offset s[f]\")\n",
    "axs[1].set_title(\"Per-frame offset convergence\")\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k3d_3d_plot(marques_factorization(W)[2]) # tomasi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.grad_search_offset import fit_scales_offsets_spectral_tail\n",
    "\n",
    "F, P = 6, 100\n",
    "shape = torch.rand(4, P)\n",
    "\n",
    "motion = torch.stack([random_affine_camera() for _ in range(F)]).reshape(-1, 4)  # [2F, 4]\n",
    "rand_lambda = random_tensor = (1.2 - 0.5) * torch.rand(F,P) + 0.5\n",
    "\n",
    "rand_W = (motion @ shape) / rand_lambda.repeat_interleave(2, dim=0)\n",
    "\n",
    "rand_lambda_hat = rand_lambda.clone()\n",
    "\n",
    "# add a random offset to each line of rand_lambda_hat\n",
    "offsets_list = [torch.rand(1).item() * 5.0 for _ in range(F)]\n",
    "for f in range(F):\n",
    "    rand_lambda_hat[f, :] += offsets_list[f]\n",
    "\n",
    "s_est, t_est, Y_corr, first_W, loss_history = fit_scales_offsets_spectral_tail(rand_W, rand_lambda_hat, r=4, iters=200, lr=1)\n",
    "\n",
    "#sv_before = torch.linalg.svdvals(first_W.detach().cpu())\n",
    "sv_before = torch.linalg.svdvals(first_W)\n",
    "sv_after  = torch.linalg.svdvals(Y_corr)\n",
    "print(\"s:\", s_est)\n",
    "print(\"t:\", t_est)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sv_before.cpu().numpy(), \"-x\", label=\"before\")\n",
    "plt.plot(sv_after.cpu().numpy(), \"-o\", label=\"after\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "for f in range(F):\n",
    "    plt.plot(s_est[:, f].cpu().numpy(), \"-o\", label=f\"frame {_}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vggt = torch.linalg.svdvals(W_vggt_nohomg)\n",
    "plt.plot(s_vggt[:20] /  s_vggt[0], \"-o\", label=\"vggt\")\n",
    "\n",
    "s_unidepth = torch.linalg.svdvals(W_uni_nohomg)\n",
    "plt.plot(s_unidepth[:20] / s_unidepth[0], \"-x\", label=\"unidepth\")\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Singular Values Comparison VGGt vs UniDepth, normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tracks_lambda_vggt[0,:].flatten() , label=\"vggt\")\n",
    "plt.plot(tracks_lambda_unidepth[0,:].flatten(), label=\"unidepth\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "for f in range(tracks_lambda_unidepth.shape[0]):\n",
    "    plt.plot(tracks_lambda_unidepth[f,:].flatten() , label=f\"frame {f}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "scales = []\n",
    "offsets = []\n",
    "\n",
    "\n",
    "for frame in range(tracks_lambda_vggt.shape[0]):\n",
    "    x = tracks_lambda_vggt[frame, :].flatten()\n",
    "    y = tracks_lambda_unidepth[frame, :].flatten()\n",
    "    A = torch.stack([x, torch.ones_like(x)], dim=1)  # [N,2]\n",
    "    sol = torch.linalg.lstsq(A, y.unsqueeze(1)).solution\n",
    "    s, o = sol.squeeze()\n",
    "    #o = torch.tensor(0)\n",
    "\n",
    "    scales.append(s.item())\n",
    "    offsets.append(o.item())\n",
    "\n",
    "    y_pred = s * x + o\n",
    "    residuals = y - y_pred\n",
    "    rmse = torch.sqrt(torch.mean(residuals**2))\n",
    "    corr = torch.corrcoef(torch.stack([x, y]))[0, 1]\n",
    "    print(f\"RMSE: {rmse.item():.4f}, correlation: {corr.item():.4f}\")\n",
    "\n",
    "    if 0:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        # Left subplot: Video frame with residuals overlay\n",
    "        axes[0].imshow((video_tensor_original[0,frame] + 1) / 2)\n",
    "        xs = obs_mat[frame * 2 + 0, :]\n",
    "        ys = obs_mat[frame * 2 + 1, :]\n",
    "        scatter = axes[0].scatter(xs.flatten(), ys.flatten(), c=np.abs(residuals.flatten()), cmap='hot', s=10)\n",
    "        fig.colorbar(scatter, ax=axes[0], label='Residuals')\n",
    "        axes[0].set_title(\"Residuals Visualization\")\n",
    "\n",
    "        # Right subplot: Residuals plot\n",
    "        axes[1].plot(residuals.numpy(), label=\"Residuals\")\n",
    "        axes[1].set_title(\"Residuals Plot\")\n",
    "        axes[1].set_xlabel(\"Feature Index\")\n",
    "        axes[1].set_ylabel(\"Residual Value\")\n",
    "        axes[1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plt.plot(scales)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.projective_factorization import sample_depths\n",
    "\n",
    "def backproject_points(tracks, depths, Ks):\n",
    "    \"\"\"\n",
    "    Backproject 2D points to 3D using per-frame intrinsics.\n",
    "\n",
    "    Args:\n",
    "        tracks: [2F, P] pixel coords (u, v interleaved per frame)\n",
    "        depths: [F, P] sampled depths for each feature\n",
    "        Ks: [F, 3, 3] Intrinsic matrices for each frame (or [1, 3, 3] for shared intrinsics)\n",
    "\n",
    "    Returns:\n",
    "        [F, P, 3] 3D points in camera coords per frame\n",
    "    \"\"\"\n",
    "    F, P = depths.shape\n",
    "    \n",
    "    # Extract intrinsics and reshape to [F, 1] (or [1, 1]) for broadcasting\n",
    "    fx = Ks[:, 0, 0].unsqueeze(1)\n",
    "    fy = Ks[:, 1, 1].unsqueeze(1)\n",
    "    cx = Ks[:, 0, 2].unsqueeze(1)\n",
    "    cy = Ks[:, 1, 2].unsqueeze(1)\n",
    "\n",
    "    # Separate u and v coordinates\n",
    "    # tracks is [2F, P], so we slice stride 2 to get [F, P]\n",
    "    u = tracks[0::2, :] \n",
    "    v = tracks[1::2, :]\n",
    "\n",
    "    # Apply pinhole inverse projection\n",
    "    # Shapes: ([F, P] - [F, 1]) / [F, 1] * [F, P]\n",
    "    X = (u - cx) / fx * depths\n",
    "    Y = (v - cy) / fy * depths\n",
    "    Z = depths\n",
    "\n",
    "    return torch.stack([X, Y, Z], dim=-1)  # [F, P, 3]\n",
    "\n",
    "num_frames = uni_depth_tensor.shape[0] \n",
    "\n",
    "tracks_depth = sample_depths(uni_depth_tensor[0:num_frames,...], W_uni[0:num_frames*2,...])  # [F, P]\n",
    "\n",
    "pts = backproject_points(W_uni[0:num_frames*2,...], tracks_depth,\n",
    "                          Ks=uni_focal_tensor) # all frames\n",
    "\n",
    "X,Y,Z = pts[:,0].cpu(), pts[:,1].cpu(), pts[:,2].cpu()\n",
    "\n",
    "#use rerrun to see the pts\n",
    "k3d_3d_plot(pts.T, scale=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliar.depth_tensor_viz import plot_depth_tensor_grid\n",
    "plot_depth_tensor_grid(uni_depth_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find fundamental matrices between frames\n",
    "from tqdm import tqdm\n",
    "\n",
    "fig, axes = plt.subplots(3,2, figsize=(15, 10))  # Create a 2x3 grid\n",
    "axes = axes.flatten()\n",
    "\n",
    "for frame in range(obs_mat.shape[0] // 2 - 1):\n",
    "\n",
    "    pts1 = obs_mat[frame*2:frame*2+2, :].T\n",
    "    pts2 = obs_mat[(frame+8)*2:(frame+8)*2+2, :].T\n",
    "\n",
    "    F, mask = cv.findFundamentalMat(\n",
    "        pts1.cpu().numpy(), \n",
    "        pts2.cpu().numpy(), \n",
    "        cv.FM_RANSAC,\n",
    "        ransacReprojThreshold=1.0)\n",
    "    \n",
    "    # show both imaegs side by side and represent the outliers\n",
    "    img1 = ((video[frame] + 1) / 2.0)\n",
    "    img2 = ((video[frame+8] + 1) / 2.0)\n",
    "    combined_img = np.hstack((img1, img2))\n",
    "    axes[frame].imshow(combined_img)\n",
    "\n",
    "    for i in range(0, pts1.shape[0], 10):\n",
    "        if mask[i]:\n",
    "            color = 'g'  # Inlier\n",
    "        else:\n",
    "            color = 'r'  # Outlier\n",
    "        axes[frame].scatter(pts1[i, 0], pts1[i, 1], s=6, c=color)\n",
    "        axes[frame].scatter(pts2[i, 0] + img1.shape[1], pts2[i, 1], s=6, c=color)  # Offset x for second image\n",
    "        axes[frame].set_title(f\"Frame {frame+1} and {frame+2}\")\n",
    "\n",
    "    break  # only first pair for demo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
