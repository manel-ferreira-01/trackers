{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import mediapy\n",
    "import pickle\n",
    "import torch\n",
    "from scipy.io import savemat\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from auxiliar.tracking_viz import generate_frame_image_cv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from src.ortho_factorization import marques_factorization\n",
    "import rerun as rr\n",
    "from src.mat_compl import alternating_matrix_completion\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from auxiliar.read_video import read_video_or_images\n",
    "from src.tapnext_infer import init_tapnext\n",
    "\n",
    "model_video_size = (256, 256)\n",
    "device = \"cuda:0\"\n",
    "\n",
    "#path = \"../mast3r/images_in/casa_bea_4_photos\"\n",
    "path = \"../mast3r/images_in/lixo_lab.mp4\"\n",
    "#video_resized, video = read_video_or_images(\"../mast3r/images_in/bordalo.mp4\",model_video_size, device)\n",
    "\n",
    "video_square, _ = read_video_or_images(path)\n",
    "print(video_square.shape)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the query points array in the original video size\n",
    "step = 70\n",
    "ys, xs = np.meshgrid(np.linspace(8, video_square[0].shape[0]-step, step), np.linspace(8, video_square[0].shape[1]-step, step))\n",
    "query_points_initial = torch.tensor(\n",
    "    np.stack(\n",
    "    [np.zeros(len(xs.flatten())), xs.flatten(), ys.flatten()], axis=1\n",
    ")[None],dtype=torch.float32).to(device)\n",
    "\n",
    "#resize the querys points to the model size\n",
    "query_points_initial[0, :, 1:3] /= torch.tensor([video_square[0].shape[1] / model_video_size[0], video_square[0].shape[0] / model_video_size[1]]).to(device) # 1, N, 3(frames,x,y)\n",
    "\n",
    "#creata tensors for the video information\n",
    "frame_cut = max(20, len(video_square))\n",
    "#frame_cut = 106\n",
    "frame_skip = 7\n",
    "video_tensor = torch.cat([(frame).unsqueeze(0) for frame in video_square]).unsqueeze(0)[:,0:frame_cut:frame_skip,...] # 1, time, H, W, 3\n",
    "video_tensor = torch.clip(video_tensor, -1, 1)\n",
    "num_frames = video_tensor.shape[1]\n",
    "\n",
    "print(\"frames:\", num_frames)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tapnext_infer import run_tapnext, init_alltracker\n",
    "from src.new_queries import add_new_tracks\n",
    "from auxiliar.read_video import resize_to_max_side\n",
    "import utils.saveload\n",
    "import utils.basic\n",
    "import utils.improc\n",
    "\n",
    "mode = \"cotracker\"  # choose: \"alltracker\", \"tapnext\", \"cotracker\"\n",
    "\n",
    "match mode:\n",
    "    # ==============================================================\n",
    "    case \"alltracker\":\n",
    "\n",
    "        model = init_alltracker(device)\n",
    "\n",
    "        rgbs = resize_to_max_side((video_tensor.permute(0,1,4,2,3) + 1) / 2 * 255)\n",
    "        B, T, C, H, W = rgbs.shape\n",
    "        \n",
    "        flows_e, visconf_maps_e, _, _ = model.forward_sliding(\n",
    "            rgbs[:, 0:], iters=4, sw=None, is_training=False\n",
    "        )\n",
    "\n",
    "        # grid in pixel coords\n",
    "        grid_xy = utils.basic.gridcloud2d(1, H, W, norm=False, device=\"cpu\").float()  # [1,H*W,2]\n",
    "        grid_xy = grid_xy.permute(0, 2, 1).reshape(1, 1, 2, H, W)  # [1,1,2,H,W]\n",
    "\n",
    "        # flows_e: [1, T, 2, H, W]\n",
    "        # grid_xy: [1, 1, 2, H, W]\n",
    "        traj_maps_e = flows_e + grid_xy  # [1, T, 2, H, W]\n",
    "\n",
    "        # → [1, T, H, W, 2]\n",
    "        traj_maps_e = traj_maps_e.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "        # get forward visibility confidence (channel 1)\n",
    "        vis_forward = visconf_maps_e[:, :, 1, :, :]  # [1, T, H, W]\n",
    "\n",
    "        # apply a threshold (e.g. > 0.5)\n",
    "        visible_mask = vis_forward > 0.75             # [1, T, H, W]\n",
    "        visible_mask = visible_mask.unsqueeze(-1)    # [1, T, H, W, 1]\n",
    "        visible_mask = visible_mask.expand_as(traj_maps_e)  # [1, T, H, W, 2]\n",
    "\n",
    "        traj_maps_e[~visible_mask] = float('nan')\n",
    "\n",
    "\n",
    "        # get original video shape\n",
    "        H_orig, W_orig = video_tensor.shape[2:4]\n",
    "        scale_x = W_orig / W  # width ratio\n",
    "        scale_y = H_orig / H  # height ratio\n",
    "        scale = torch.tensor([scale_x, scale_y], device=traj_maps_e.device)\n",
    "\n",
    "        # apply before flattening\n",
    "        traj_maps_e = traj_maps_e * scale\n",
    "\n",
    "        # flatten to [1, T, H*W, 2]\n",
    "        output_tensor = traj_maps_e.reshape(1, traj_maps_e.shape[1], H * W, 2)  # [1, T, N, 2]\n",
    "\n",
    "    # ==============================================================\n",
    "    case \"tapnext\":\n",
    "        \n",
    "        tapnext = init_tapnext(device, \"/home/manuelf/storage/bootstapnext_ckpt.npz\")\n",
    "        print(\"Model initialized\")\n",
    "\n",
    "        track_histories = run_tapnext(\n",
    "            video_tensor,  # send the video resized to the model size\n",
    "            query_points_initial,\n",
    "            tapnext,\n",
    "            device=device,\n",
    "            new_tracks_flag=False,  # or None\n",
    "        )\n",
    "        output = {}\n",
    "\n",
    "        for track_id, trajectory in tqdm(track_histories.items()):  # per feature\n",
    "            coords = []\n",
    "            for t in range(num_frames):  # per frame\n",
    "                step = next((pos for (frame, pos) in trajectory if frame == t), None)\n",
    "                coords.append(step if step is not None else torch.tensor([float('nan'), float('nan')]))\n",
    "            output[track_id] = torch.stack(coords)  # shape: [num_frames, 2]\n",
    "\n",
    "        output_list = [trajectory.unsqueeze(0) for _, trajectory in output.items()]\n",
    "        output_tensor = torch.cat(output_list, dim=0).unsqueeze(0).permute(0, 2, 1, 3)  # [1, num_frames, num_feats, 2]\n",
    "\n",
    "        # Flip x/y and scale back to original video size\n",
    "        output_tensor = output_tensor[:, :, :, [1, 0]]\n",
    "        output_tensor *= torch.tensor([\n",
    "            video_tensor[0].shape[1] / model_video_size[0],\n",
    "            video_tensor[0].shape[0] / model_video_size[1]\n",
    "        ])\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # ==============================================================\n",
    "    case \"cotracker\":\n",
    "        cotracker = torch.hub.load(\"facebookresearch/co-tracker\", \"cotracker3_offline\").to(device)\n",
    "\n",
    "        pred_tracks, pred_visibility = cotracker(\n",
    "            video_tensor.squeeze().permute(0, 3, 1, 2).unsqueeze(0).float().to(device),\n",
    "            grid_size=step,\n",
    "            thr=0.9,\n",
    "        )  # B T N 2,  B T N 1\n",
    "\n",
    "        pred_tracks = pred_tracks.cpu()\n",
    "        pred_visibility = pred_visibility.cpu()\n",
    "\n",
    "        # Ensure shape [1, T, N, 1]\n",
    "        if pred_visibility.ndim == 3:\n",
    "            pred_visibility = pred_visibility.unsqueeze(-1)\n",
    "\n",
    "        # Visibility mask\n",
    "        visible_mask = pred_visibility.expand_as(pred_tracks)\n",
    "        pred_tracks_masked = pred_tracks.clone()\n",
    "        pred_tracks_masked[~visible_mask] = float('nan')\n",
    "\n",
    "        # Optional flip if you want consistency\n",
    "        # pred_tracks_masked = pred_tracks_masked[:, :, :, [1, 0]]\n",
    "\n",
    "        output_tensor = pred_tracks_masked  # [1, num_frames, num_feats, 2]\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # ==============================================================\n",
    "    case _:\n",
    "        raise ValueError(f\"Unknown tracking mode: {mode}\")\n",
    "\n",
    "# Unified final output: output_tensor → shape [1, num_frames, num_feats, 2]\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frame_cut_out = output_tensor.shape[1]\n",
    "\n",
    "# Calculate the number of visible frames for each track\n",
    "visible_counts = torch.sum(~torch.isnan(output_tensor[0, :frame_cut_out, :, 0]), dim=0) #shape: #feats\n",
    "\n",
    "# Filter tracks that are visible for at least 20 frames\n",
    "visible_tracks_mask = visible_counts >= 1\n",
    "output_tensor_filtered = output_tensor[:, :frame_cut_out, visible_tracks_mask, :]\n",
    "output_tensor_filtered.shape\n",
    "\n",
    "video_tensor_original = video_tensor.to(\"cpu\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_frame_image_cv(video_tensor_original.cpu(), output_tensor[:,:,::10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from src.interpolations import get_rgb_at_coords_torch\n",
    "\n",
    "# output_tensor_filtered: [1, num_frames, num_feats, 2]\n",
    "num_frames = output_tensor_filtered.shape[1]\n",
    "num_feats = output_tensor_filtered.shape[2]\n",
    "\n",
    "# Build observation matrix: [feats*2, frames]\n",
    "obs_mat_full = torch.full((num_frames * 2, num_feats), float(\"nan\"), device=output_tensor_filtered.device)\n",
    "\n",
    "for frame in range(num_frames):  # per frame\n",
    "    obs_mat_full[frame*2, :] = output_tensor_filtered[0, frame, :, 0]  # x\n",
    "    obs_mat_full[frame*2+1, :] = output_tensor_filtered[0, frame, :, 1]  # y\n",
    "\n",
    "# remove columns (features) that have any NaN values\n",
    "valid_columns_mask_no_nan = ~torch.isnan(obs_mat_full).any(dim=0)\n",
    "obs_mat = obs_mat_full[:, valid_columns_mask_no_nan]\n",
    "\n",
    "# subsample columns (features) by a factor of 50\n",
    "#obs_mat = obs_mat[:, ::10]\n",
    "\n",
    "# remove columns with more than ~50% NaN values\n",
    "nan_counts = torch.isnan(obs_mat_full).sum(dim=0)\n",
    "valid_columns_mask_half = nan_counts < (obs_mat_full.shape[0] / 1.3)\n",
    "obs_mat_half_missed = obs_mat_full[:, valid_columns_mask_half]\n",
    "\n",
    "# percentage of missing values\n",
    "missing_percentage = (\n",
    "    torch.isnan(obs_mat_half_missed).sum().float()\n",
    "    / (obs_mat_half_missed.numel())\n",
    "    * 100\n",
    ").item()\n",
    "\n",
    "print(f\"Percentage of missing values: {missing_percentage:.2f}%\")\n",
    "\n",
    "# plot with torch → numpy just for visualization\n",
    "#plt.figure(figsize=(12, 6))\n",
    "#plt.imshow((obs_mat_half_missed), aspect=\"auto\", cmap=\"gray\" )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# obs_mat: [2*frames, feats]\n",
    "# video: [frames, H, W, C] in [-1, 1] (assumed)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))  # Create a 2x3 grid\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Predefined features (convert to torch if you want consistency)\n",
    "features = torch.tensor([3, 9, 315, 227, 266, 207, 107])\n",
    "features = torch.tensor([3, 9])\n",
    "rand_feats = torch.randint(0, obs_mat.shape[1], (20,))\n",
    "\n",
    "features = torch.cat([features, rand_feats])\n",
    "print(\"Selected features:\", features.tolist())\n",
    "\n",
    "# Loop through frames\n",
    "cnt = 0\n",
    "for i in torch.linspace(0, video_tensor.cpu().shape[1]-1, 6).to(torch.int):  # num_frames\n",
    "    # Convert frame for plotting\n",
    "    frame_img = ((video_tensor[0,i].cpu() + 1) / 2.0)\n",
    "    axes[cnt].imshow(frame_img)\n",
    "\n",
    "    axes[cnt].scatter(obs_mat[i*2, features], obs_mat[i*2+1, features], s=6, c=\"red\")\n",
    "    axes[cnt].set_title(f\"Frame {i+1}\")\n",
    "    axes[cnt].axis(\"off\")\n",
    "    cnt += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ortho_factorization import costeira_marques\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "obs_completed, _ = alternating_matrix_completion(obs_mat, mode=0, max_iters=200, rank=3)\n",
    "# completion + factorization\n",
    "motion, shape, t = costeira_marques(obs_completed) # rank 3\n",
    "reconstructed_obs = motion @ shape + t.repeat(1, shape.shape[1])\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot the completed observation matrix\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(obs_completed / np.nanmax(obs_completed), aspect='auto', cmap='gray')\n",
    "plt.title(\"Rank constrained Observation Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot the original observation matrix\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(obs_mat / np.nanmax(obs_mat), aspect='auto', cmap='gray')\n",
    "plt.title(\"Original Observation Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot the singular values\n",
    "plt.subplot(1, 3, 3)\n",
    "singular_values_rank = torch.linalg.svd(obs_completed, full_matrices=False).S\n",
    "plt.plot(singular_values_rank.cpu().numpy(), \"-x\", linewidth=2)\n",
    "\n",
    "singular_values_original = torch.linalg.svd(obs_mat, full_matrices=False).S\n",
    "plt.plot(singular_values_original.cpu().numpy(), \"-o\")\n",
    "plt.legend([\"rank_constrained\", \"Original\"])\n",
    "plt.title(\"Singular Values\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "#plot the singular values of the reconstructed\n",
    "plt.figure(figsize=(10, 6))\n",
    "singular_values_reconstructed = torch.linalg.svd(reconstructed_obs, full_matrices=False).S\n",
    "plt.plot(singular_values_reconstructed.cpu().numpy(), \"-o\", linewidth=2)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Singular Values of Reconstructed Observation Matrix\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliar.depth_tensor_viz import k3d_3d_plot\n",
    "k3d_3d_plot(shape.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "sys.path.append('../boxes/vggt/protos')\n",
    "import pipeline_pb2 as vggt_pb2\n",
    "import pipeline_pb2_grpc as vggt_pb2_grpc\n",
    "from aux import wrap_value, unwrap_value\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "image_byte_list = []\n",
    "# List all files in the directory\n",
    "if 0:\n",
    "    \n",
    "    files = [os.path.join(path, file) for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n",
    "\n",
    "    for file in sorted(files):\n",
    "        # Open the file in binary read mode ('rb') and read its entire content\n",
    "        with open(file, 'rb') as f:\n",
    "            image_bytes = f.read()\n",
    "            image_byte_list.append(image_bytes)\n",
    "            print(f\"Read {file}: {len(image_bytes) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "else:\n",
    "\n",
    "    # Convert video frames (tensor) to a list of byte arrays\n",
    "    for frame in video_tensor[0]:  # Assuming video_tensor_original is [1, time, H, W, C]\n",
    "        # Convert tensor to numpy array and scale to [0, 255]\n",
    "        frame_np = (frame.numpy() * 255).astype('uint8')\n",
    "        # Create an Image object\n",
    "        img = Image.fromarray(frame_np)\n",
    "        # Save the image to a bytes buffer\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"JPEG\")\n",
    "        # Append the bytes to the list\n",
    "        image_byte_list.append(buffer.getvalue())\n",
    "\n",
    "config_json = {\n",
    "        \"command\": \"3d_infer\",\n",
    "        \"parameters\": {\n",
    "            \"device\": \"cuda:0\", # TODO: implement this\n",
    "            \"conf_vis\": 50.0\n",
    "        }\n",
    "}\n",
    "\n",
    "request = vggt_pb2.Envelope(data={\"images\":wrap_value(image_byte_list)},\n",
    "                            config_json = json.dumps(config_json))\n",
    "channel_opt = [('grpc.max_send_message_length', -1), ('grpc.max_receive_message_length', -1)]\n",
    "channel=grpc.insecure_channel(\"localhost:8061\",options=channel_opt)\n",
    "estimator_stub = vggt_pb2_grpc.PipelineServiceStub(channel)\n",
    "response = estimator_stub.Process(request)\n",
    "channel.close()\n",
    "\n",
    "#write the glb as a file\n",
    "glb_file = unwrap_value(response.data[\"glb_file\"])\n",
    "with open(\"output.glb\", \"wb\") as f:\n",
    "    f.write(glb_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliar.depth_tensor_viz import k3d_3d_plot, plot_depth_tensor_grid\n",
    "wrld_pts = np.load(io.BytesIO(unwrap_value(response.data[\"world_points\"]))).squeeze().reshape(-1,3)\n",
    "wrld_conf = np.load(io.BytesIO(unwrap_value(response.data[\"world_points_conf\"]))).squeeze().reshape(-1,1)\n",
    "                    \n",
    "k3d_3d_plot(wrld_pts[wrld_conf.flatten() > 1, :][::10000].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_depth_tensor_grid(torch.tensor(np.load(io.BytesIO(unwrap_value(response.data[\"depth\"])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from src.projective_factorization import build_depth_weighted_matrix, normalize_measurement_matrix\n",
    "\n",
    "vggt_depth_tensor = torch.tensor(np.load(io.BytesIO(unwrap_value(response.data[\"depth\"]))))  # 1, time, H, W, 1\n",
    "vggt_conf_tensor = torch.tensor(np.load(io.BytesIO(unwrap_value(response.data[\"depth_conf\"]))))  # 1, time, H, W, 1\n",
    "instrisics = torch.tensor(np.load(io.BytesIO(unwrap_value(response.data[\"intrinsic\"]))))\n",
    "extrinsics = torch.tensor(np.load(io.BytesIO(unwrap_value(response.data[\"extrinsic\"]))))\n",
    "wrld_pts = torch.tensor(np.load(io.BytesIO(unwrap_value(response.data[\"world_points\"])))) # 1, #frame, H, W, 3\n",
    "\n",
    "# need to resize the observation matrix to the size of the depth tensor\n",
    "W_vggt = obs_mat.clone()\n",
    "W_vggt[0::2,:] *=  (vggt_depth_tensor.shape[2] / video_tensor.shape[2])\n",
    "W_vggt[1::2,:] *=  (vggt_depth_tensor.shape[3] / video_tensor.shape[3])\n",
    "\n",
    "# average all intrinsic matrices\n",
    "K_vggt = torch.mean(instrisics, dim=0) # TODO: this needs to be resized due to the tracks\n",
    "#K = torch.eye(3)\n",
    "\n",
    "frames_to_use = vggt_depth_tensor.shape[1]\n",
    "W_depth_vggt, tracks_lambda_vggt = build_depth_weighted_matrix(W_vggt[0:frames_to_use*2,:], vggt_depth_tensor.squeeze()[0:frames_to_use,:],\n",
    "                                            fx=K_vggt[0,0], fy=K_vggt[1,1], cx=K_vggt[0,2], cy=K_vggt[1,2])\n",
    "\n",
    "pattern = torch.tensor([1, 1, 0], dtype=torch.bool)\n",
    "mask = pattern.repeat((W_depth_vggt.shape[0] + len(pattern) - 1) // len(pattern))[:W_depth_vggt.shape[0]]\n",
    "W_vggt_nohomg = W_depth_vggt[mask, ...]\n",
    "\n",
    "# Normalize the measurement matrix with isotropic scaling sqrt(3)\n",
    "Wn, T_list = normalize_measurement_matrix(W_depth_vggt)                     # Wn: [3F, P]\n",
    "\n",
    "U, S, Vh = torch.linalg.svd(W_depth_vggt, full_matrices=False)\n",
    "r = 4\n",
    "Ur = U[:, :r]\n",
    "Sr = torch.diag(S[:r])\n",
    "Vr = Vh[:r, :]\n",
    "\n",
    "Ssqrt = torch.linalg.cholesky(Sr)  \n",
    "Motion = Ur @ Ssqrt                     \n",
    "Sshape = Ssqrt @ Vr                # (4,  P) \n",
    "W_hat = Motion @ Sshape\n",
    "\n",
    "#X3d = Sshape[:3, :] / Sshape[3:4, :]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the normalized measurement matrix\n",
    "im1 = axes[0].imshow(W_depth_vggt, aspect='auto', cmap='gray', interpolation='none')\n",
    "axes[0].set_title(\"Normalized Measurement Matrix\")\n",
    "fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Plot the singular values in log scale\n",
    "axes[1].plot(torch.linalg.svd(W_depth_vggt)[1], \"-x\")\n",
    "axes[1].set_xlabel(\"Index\")\n",
    "axes[1].set_title(\"Log10 Singular Values\")\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization of the W_hat with the no homogenous coordinates and the lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.projective_factorization import metric_upgrade_daq\n",
    "U, S, V = torch.linalg.svd(W_depth_vggt)\n",
    "\n",
    "r = 4\n",
    "Ur = U[:, :r]\n",
    "Sr = torch.diag(S[:r])\n",
    "Vr = Vh[:r, :]\n",
    "\n",
    "Ssqrt = torch.linalg.cholesky(Sr)  \n",
    "Motion = Ur @ Ssqrt                     \n",
    "Shape = Ssqrt @ Vr\n",
    "\n",
    "H, P_e, R_list, t_list = metric_upgrade_daq(Motion, [K_vggt for _ in range(W_depth_vggt.shape[0] // 3)])\n",
    "\n",
    "X_e = H @ Shape       # Shape = 4×P from your projective factorization\n",
    "X_e = X_e / X_e[3]\n",
    "theta = lambda R :torch.arccos(((torch.trace(R) - 1) / 2).clamp(-1, 1))\n",
    "R_norm = [theta(R) for R in R_list]\n",
    "R_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def camera_centers_and_axes(R_list, t_list, axis_len=0.2):\n",
    "    centers, axes = [], []\n",
    "    for R, t in zip(R_list, t_list):\n",
    "        # camera center in world frame\n",
    "        C = -R.T @ t\n",
    "        centers.append(C)\n",
    "        # camera axes in world frame (columns of R)\n",
    "        axes.append(R.T * axis_len)   # 3×3, each column = world direction of local axis\n",
    "    return torch.stack(centers), torch.stack(axes)\n",
    "\n",
    "def plot_cameras_3d(R_list, t_list, X_e=None, axis_len=0.2, text_offset=0.03):\n",
    "    \"\"\"\n",
    "    Plot camera centers and orientation axes in 3D.\n",
    "\n",
    "    Args:\n",
    "        R_list: list of (3,3) rotation matrices\n",
    "        t_list: list of (3,) translations\n",
    "        X_e: optional (4,P) homogeneous 3D points\n",
    "        axis_len: length of drawn axes for each camera\n",
    "        text_offset: offset for axis labels\n",
    "    \"\"\"\n",
    "    assert len(R_list) == len(t_list)\n",
    "    F = len(R_list)\n",
    "\n",
    "    # compute camera centers and axis directions in world coords\n",
    "    centers = []\n",
    "    axes = []\n",
    "    for R, t in zip(R_list, t_list):\n",
    "        C = -R.T @ t               # camera center\n",
    "        centers.append(C)\n",
    "        axes.append(R.T * axis_len)  # world directions of local axes\n",
    "    centers = torch.stack(centers)\n",
    "    axes = torch.stack(axes)\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # optional points\n",
    "    if X_e is not None:\n",
    "        X = X_e[:3] / X_e[3]\n",
    "        ax.scatter(X[0], X[1], X[2], c='gray', s=2, alpha=0.4, label='points')\n",
    "\n",
    "    # camera centers\n",
    "    ax.scatter(centers[:,0], centers[:,1], centers[:,2], c='k', s=25, label='camera centers')\n",
    "\n",
    "    for i, (C, A) in enumerate(zip(centers, axes)):\n",
    "        # each column of A is the world direction of local axis\n",
    "        ax.plot([C[0], C[0]+A[0,0]], [C[1], C[1]+A[0,1]], [C[2], C[2]+A[0,2]], 'r')  # X\n",
    "        ax.plot([C[0], C[0]+A[1,0]], [C[1], C[1]+A[1,1]], [C[2], C[2]+A[1,2]], 'g')  # Y\n",
    "        ax.plot([C[0], C[0]+A[2,0]], [C[1], C[1]+A[2,1]], [C[2], C[2]+A[2,2]], 'b')  # Z\n",
    "\n",
    "        # camera index label near center\n",
    "        ax.text(C[0], C[1], C[2], f'{i}', color='k', fontsize=8, weight='bold')\n",
    "\n",
    "    # formatting\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_box_aspect([1,1,1])\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return ax\n",
    "\n",
    "ax = plot_cameras_3d(R_list, t_list,axis_len=0.05)\n",
    "ax.axis(\"equal\")\n",
    "\n",
    "#X_cart = X_e[:3] / X_e[3]\n",
    "#ax.scatter(X_cart[0], X_cart[1], X_cart[2], c='gray', s=2, alpha=0.5, label='3D points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another depth estimator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/home/manuelf/UniDepth/\")\n",
    "from unidepth.models import UniDepthV2\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model = UniDepthV2.from_pretrained(\"lpiccinelli/unidepth-v2-vitl14\").to(device) # or \"lpiccinelli/unidepth-v1-cnvnxtl\" for the ConvNext backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "uni_conf = []\n",
    "uni_focal = []\n",
    "\n",
    "for frame in tqdm(range(video_tensor.shape[1])):\n",
    "    pred = model.infer(video_tensor[0,frame].permute(2,0,1).to(device))\n",
    "    #rnd = torch.rand(1).to(device) * 100\n",
    "    rnd = 1\n",
    "    #print(rnd)\n",
    "    depths.append(pred[\"depth\"] * rnd)\n",
    "    uni_conf.append(1 / pred[\"confidence\"])\n",
    "    uni_focal.append(pred[\"intrinsics\"])\n",
    "\n",
    "uni_depth_tensor = torch.stack(depths).unsqueeze(-1).cpu().squeeze()  # (num_frames, H, W, 1)\n",
    "uni_conf_tensor = torch.stack(uni_conf).unsqueeze(-1).cpu().squeeze()  # (num_frames, H, W, 1)\n",
    "uni_focal_tensor = torch.stack(uni_focal).cpu().squeeze()  # (num_frames, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.projective_factorization import build_depth_weighted_matrix, normalize_measurement_matrix\n",
    "\n",
    "# make the tracks fit into the new depth size\n",
    "W_uni = obs_mat.clone()\n",
    "W_uni[0::2,:] = W_uni[0::2] * (uni_depth_tensor.shape[1] / video_tensor.shape[2])\n",
    "W_uni[1::2,:] = W_uni[1::2] * (uni_depth_tensor.shape[2] / video_tensor.shape[3])\n",
    "\n",
    "K_uni_mean = torch.mean(uni_focal_tensor, dim=0)\n",
    "#K = torch.eye(3)\n",
    "\n",
    "frames_to_use = uni_depth_tensor.shape[0]\n",
    "W_depth_uni, tracks_lambda_unidepth = build_depth_weighted_matrix(W_uni[0:frames_to_use*2,:], uni_depth_tensor[0:frames_to_use,:],\n",
    "                                            fx=K_uni_mean[0,0], fy=K_uni_mean[1,1], cx=K_uni_mean[0,2], cy=K_uni_mean[1,2])\n",
    "\n",
    "pattern = torch.tensor([1, 1, 0], dtype=torch.bool)\n",
    "mask = pattern.repeat((W_depth_uni.shape[0] + len(pattern) - 1) // len(pattern))[:W_depth_uni.shape[0]]\n",
    "W_uni_nohomg = W_depth_uni[mask, ...]\n",
    "\n",
    "Wn, T_list = normalize_measurement_matrix(W_depth_uni)                     # Wn: [3F, P]\n",
    "\n",
    "U, S, Vh = torch.linalg.svd(Wn, full_matrices=False)\n",
    "r = 4\n",
    "Ur = U[:, :r]\n",
    "Sr = torch.diag(S[:r])\n",
    "Vr = Vh[:r, :]\n",
    "\n",
    "Ssqrt = torch.linalg.cholesky(Sr)  \n",
    "Motion = Ur @ Ssqrt                     \n",
    "Sshape = Ssqrt @ Vr                # (4,  P) \n",
    "W_hat = Motion @ Sshape\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the normalized measurement matrix\n",
    "im1 = axes[0].imshow(W_depth_uni, aspect='auto', cmap='gray', interpolation='none')\n",
    "axes[0].set_title(\"Normalized Measurement Matrix\")\n",
    "fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Plot the singular values in log scale\n",
    "axes[1].plot(torch.linalg.svd(W_uni_nohomg)[1], \"-x\")\n",
    "axes[1].set_xlabel(\"Index\")\n",
    "#axes[1].set_title(\"Log10 Singular Values\")\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliar.depth_tensor_viz import k3d_3d_plot\n",
    "k3d_3d_plot(costeira_marques(W_vggt_nohomg)[1]) # tomasi\n",
    "#plt.imshow(costeira_marques(W_uni_nohomg)[1], aspect='auto', cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ortho_factorization import calibrate_orthographic, random_affine_camera\n",
    "\n",
    "if 0:\n",
    "\n",
    "    F, P = 6, 100\n",
    "    U = torch.rand(2* F, 4)\n",
    "    S = torch.diag(torch.tensor([1.0, 1.0, 1.0, 1.0]))  \n",
    "    V = torch.rand(4, P)\n",
    "    rand_final = U @ S @ V\n",
    "\n",
    "    rand_lambda = torch.rand(F, P)\n",
    "    rand_lambda_hat = rand_lambda.clone()\n",
    "\n",
    "    # offset in all frames\n",
    "    rand_lambda_hat[2,:] += 2.0   # deeper in later frames\n",
    "    rand_lambda_hat[0,:] += 1.0   # deeper in later frames\n",
    "\n",
    "    rand_lambda_expanded = rand_lambda.repeat_interleave(2, dim=0)\n",
    "    rand_W = rand_final / rand_lambda_expanded   # observed measurement\n",
    "    \n",
    "else:\n",
    "\n",
    "    F, P = 6, 100\n",
    "    shape = torch.rand(4, P)\n",
    "\n",
    "    motion = torch.stack([random_affine_camera() for _ in range(F)]).reshape(-1, 4)  # [2F, 4]\n",
    "    rand_lambda = random_tensor = (1.2 - 0.5) * torch.rand(F,P) + 0.5\n",
    "\n",
    "    rand_W = (motion @ shape) / rand_lambda.repeat_interleave(2, dim=0)\n",
    "\n",
    "    rand_lambda_hat = rand_lambda.clone()\n",
    "\n",
    "    # add a random offset to each line of rand_lambda_hat\n",
    "    offsets_list = [torch.rand(1).item() * 5.0 for _ in range(F)]\n",
    "    for f in range(F):\n",
    "        rand_lambda_hat[f, :] += offsets_list[f]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "scales, offsets, W, M, first_W, z = calibrate_orthographic(\n",
    "    rand_W, rand_lambda_hat, K=torch.eye(3),\n",
    "    iters=10, rank=4, tol=1e-6\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(torch.linalg.svdvals(first_W)[:7], \"-x\", label=\"before\")\n",
    "plt.plot(torch.linalg.svdvals(W)[:7], \"-x\", label=\"after\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10,8), sharex=True)\n",
    "\n",
    "num_iters, F = scales.shape\n",
    "iters = torch.arange(num_iters)\n",
    "\n",
    "# --- subplot 1: scales ---\n",
    "for f in range(F):\n",
    "    axs[0].plot(iters, scales[:, f].cpu(), \"-o\", label=f\"frame {f}\")\n",
    "\n",
    "axs[0].set_ylabel(\"Scale d[f]\")\n",
    "axs[0].set_title(\"Per-frame scale convergence\")\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(ncol=3, fontsize=8)\n",
    "\n",
    "# --- subplot 2: offsets ---\n",
    "for f in range(F):\n",
    "    axs[1].plot(iters, offsets[:, f].cpu()/scales[:, f].cpu(), \"-o\", label=f\"frame {f}\")\n",
    "    axs[1].hlines(-offsets_list[f], 0, iters[-1])\n",
    "\n",
    "axs[1].set_xlabel(\"Iteration\")\n",
    "axs[1].set_ylabel(\"Offset s[f]\")\n",
    "axs[1].set_title(\"Per-frame offset convergence\")\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def fit_scales_offsets_spectral_tail(W, lam_hat, r=4, iters=1000,\n",
    "                                     lr=1e-2, alpha=1e-2, beta=1e-3, device=None):\n",
    "    \"\"\"\n",
    "    W:       [2F,P]\n",
    "    lam_hat: [F,P]\n",
    "    Returns s,t and the corrected Y(s,t).\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = W.device\n",
    "    F, P = lam_hat.shape\n",
    "    assert W.shape[0] == 2*F and W.shape[1] == P\n",
    "\n",
    "    # parameters with safe reparameterization\n",
    "    s_raw = torch.zeros(F, device=device, requires_grad=True)  # s = 1 + softplus(s_raw)\n",
    "    t_raw = torch.zeros(F, device=device, requires_grad=True)  # t =     softplus/tanh choice\n",
    "\n",
    "    opt = torch.optim.Adam([s_raw, t_raw], lr=lr)\n",
    "\n",
    "    def build_Y():\n",
    "        s = 1.0 + torch.nn.functional.softplus(s_raw)          # keep s >= 1\n",
    "        t = 0.1 * torch.tanh(t_raw)                            # keep t in ~[-0.1,0.1] (adjust)\n",
    "        lam_corr = (s[:,None]*lam_hat + t[:,None]).clamp_min(1e-6)\n",
    "        Y = lam_corr.repeat_interleave(2, dim=0) * W\n",
    "        return Y, s, t\n",
    "\n",
    "    best = None\n",
    "    for _ in range(iters):\n",
    "        opt.zero_grad()\n",
    "        Y, s, t = build_Y()\n",
    "\n",
    "        # singular values\n",
    "        S = torch.linalg.svdvals(Y)\n",
    "        tail = S[r:]                                           # σ_{r+1..}\n",
    "        loss_tail = (tail**2).sum()\n",
    "\n",
    "        # gentle priors to prevent \"smashing scales\"\n",
    "        loss_reg = alpha * ((s-1)**2).sum() + beta * (t**2).sum()\n",
    "\n",
    "        loss = loss_tail + loss_reg\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # keep best by tail energy\n",
    "        with torch.no_grad():\n",
    "            if best is None or loss_tail.item() < best[0]:\n",
    "                best = (loss_tail.item(), s.detach().clone(), t.detach().clone(), Y.detach().clone())\n",
    "\n",
    "    _, s_best, t_best, Y_best = best\n",
    "    return s_best, t_best, Y_best\n",
    "\n",
    "\n",
    "F, P = 6, 100\n",
    "U = torch.randn(2*F, 4, dtype=torch.float32)\n",
    "S = torch.diag(torch.tensor([1.0, 1.0, 1.0, 1.0], dtype=torch.float32))  \n",
    "V = torch.randn(4, P, dtype=torch.float32)\n",
    "rand_final = U @ S @ V\n",
    "\n",
    "rand_lambda = torch.rand(F, P)\n",
    "rand_lambda_hat = rand_lambda.clone()\n",
    "\n",
    "# scale perturbation\n",
    "#rand_lambda_hat[-1,:] *= 3.0  # frame-10 deeper (true λ=2)\n",
    "\n",
    "# offset perturbation\n",
    "rand_lambda_hat[3,:] += 3.0   # frame-4 closer\n",
    "rand_lambda_hat[1,:] -= 10.0  # frame-4 closeewr\n",
    "\n",
    "rand_lambda_expanded = rand_lambda.repeat_interleave(2, dim=0)\n",
    "rand_W = rand_final / rand_lambda_expanded   # observed measurement\n",
    "\n",
    "s_est, t_est, Y_corr = fit_scales_offsets_spectral_tail(rand_W, rand_lambda_hat, r=4, iters=1000, lr=1)\n",
    "\n",
    "sv_before = torch.linalg.svdvals((rand_lambda_hat.repeat_interleave(2,0))*rand_W)\n",
    "sv_after  = torch.linalg.svdvals(Y_corr)\n",
    "print(\"σ5 before:\", sv_before[4].item())\n",
    "print(\"σ5 after :\", sv_after[4].item())\n",
    "print(\"s:\", s_est)\n",
    "print(\"t:\", t_est)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sv_before.cpu().numpy(), \"-x\", label=\"before\")\n",
    "plt.plot(sv_after.cpu().numpy(), \"-o\", label=\"after\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.title(\"Singular Values\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliar.depth_tensor_viz import k3d_3d_plot\n",
    "\n",
    "k3d_3d_plot(costeira_marques(res.A)[1])\n",
    "#k3d_3d_plot(costeira_marques(W_uni_nohomg)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vggt = torch.linalg.svdvals(W_vggt_nohomg)\n",
    "plt.plot(s_vggt[:20] /  s_vggt[0], \"-o\", label=\"vggt\")\n",
    "\n",
    "s_unidepth = torch.linalg.svdvals(W_uni_nohomg)\n",
    "plt.plot(s_unidepth[:20] / s_unidepth[0], \"-x\", label=\"unidepth\")\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Singular Values Comparison VGGt vs UniDepth, normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tracks_lambda_vggt[0,:].flatten() / torch.max(tracks_lambda_vggt[0,:].flatten()), label=\"vggt\")\n",
    "plt.plot(tracks_lambda_unidepth[0,:].flatten() / torch.max(tracks_lambda_unidepth[0,:].flatten()), label=\"unidepth\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "scales = []\n",
    "offsets = []\n",
    "\n",
    "\n",
    "for frame in range(5, tracks_lambda_vggt.shape[0]):\n",
    "    x = tracks_lambda_vggt[frame, :].flatten()\n",
    "    y = tracks_lambda_unidepth[frame, :].flatten()\n",
    "    A = torch.stack([x, torch.ones_like(x)], dim=1)  # [N,2]\n",
    "    sol = torch.linalg.lstsq(A, y.unsqueeze(1)).solution\n",
    "    s, o = sol.squeeze()\n",
    "\n",
    "    scales.append(s.item())\n",
    "    offsets.append(o.item())\n",
    "\n",
    "    y_pred = s * x + o\n",
    "    residuals = y - y_pred\n",
    "    rmse = torch.sqrt(torch.mean(residuals**2))\n",
    "    corr = torch.corrcoef(torch.stack([x, y]))[0, 1]\n",
    "    print(f\"RMSE: {rmse.item():.4f}, correlation: {corr.item():.4f}\")\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Left subplot: Video frame with residuals overlay\n",
    "    axes[0].imshow((video_tensor_original[0,frame] + 1) / 2)\n",
    "    xs = obs_mat[frame * 2 + 0, :]\n",
    "    ys = obs_mat[frame * 2 + 1, :]\n",
    "    scatter = axes[0].scatter(xs.flatten(), ys.flatten(), c=np.abs(residuals.flatten()), cmap='hot', s=10)\n",
    "    fig.colorbar(scatter, ax=axes[0], label='Residuals')\n",
    "    axes[0].set_title(\"Residuals Visualization\")\n",
    "\n",
    "    # Right subplot: Residuals plot\n",
    "    axes[1].plot(residuals.numpy(), label=\"Residuals\")\n",
    "    axes[1].set_title(\"Residuals Plot\")\n",
    "    axes[1].set_xlabel(\"Feature Index\")\n",
    "    axes[1].set_ylabel(\"Residual Value\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_depth_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.projective_factorization import sample_depths\n",
    "\n",
    "def backproject_points(tracks, depths, fx, fy, cx, cy):\n",
    "    \"\"\"\n",
    "    tracks: [2F, P] pixel coords\n",
    "    depths: [F, P] sampled depths for each feature\n",
    "    Returns: [F, P, 3] 3D points in camera coords per frame\n",
    "    \"\"\"\n",
    "    F, P = depths.shape\n",
    "    u = tracks[0::2, :]  # [F,P]\n",
    "    v = tracks[1::2, :]  # [F,P]\n",
    "\n",
    "    X = (u - cx) / fx * depths\n",
    "    Y = (v - cy) / fy * depths\n",
    "    Z = depths\n",
    "\n",
    "    return torch.stack([X, Y, Z], dim=-1)  # [F,P,3]\n",
    "\n",
    "num_frames = obs_mat_depth_sized.shape[0] // 2\n",
    "\n",
    "tracks_depth = sample_depths(depth_tensor[0:num_frames,...], obs_mat_depth_sized[0:num_frames*2,...])  # [F, P]\n",
    "\n",
    "K = torch.mean(instrisics, dim=0)\n",
    "\n",
    "pts = backproject_points(obs_mat_depth_sized[0:num_frames*2,...], tracks_depth,\n",
    "                          fx=K[0,0], fy=K[1,1], cx=K[0,2], cy=K[1,2]) # all frames\n",
    "\n",
    "X,Y,Z = pts[:,0].cpu(), pts[:,1].cpu(), pts[:,2].cpu()\n",
    "\n",
    "#use rerrun to see the pts\n",
    "rr.init(\"3d_backprojection\")\n",
    "rr.notebook_show(width=int(1920*0.75), height=int(1080*0.75))\n",
    "rr.log(\"world/3d_completed\", rr.Points3D(pts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliar.depth_tensor_viz import plot_depth_tensor_grid\n",
    "plot_depth_tensor_grid(vggt_depth_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find fundamental matrices between frames\n",
    "from tqdm import tqdm\n",
    "\n",
    "fig, axes = plt.subplots(3,2, figsize=(15, 10))  # Create a 2x3 grid\n",
    "axes = axes.flatten()\n",
    "\n",
    "for frame in range(obs_mat.shape[0] // 2 - 1):\n",
    "\n",
    "    pts1 = obs_mat[frame*2:frame*2+2, :].T\n",
    "    pts2 = obs_mat[(frame+8)*2:(frame+8)*2+2, :].T\n",
    "\n",
    "    F, mask = cv.findFundamentalMat(\n",
    "        pts1.cpu().numpy(), \n",
    "        pts2.cpu().numpy(), \n",
    "        cv.FM_RANSAC,\n",
    "        ransacReprojThreshold=1.0)\n",
    "    \n",
    "    # show both imaegs side by side and represent the outliers\n",
    "    img1 = ((video[frame] + 1) / 2.0)\n",
    "    img2 = ((video[frame+8] + 1) / 2.0)\n",
    "    combined_img = np.hstack((img1, img2))\n",
    "    axes[frame].imshow(combined_img)\n",
    "\n",
    "    for i in range(0, pts1.shape[0], 10):\n",
    "        if mask[i]:\n",
    "            color = 'g'  # Inlier\n",
    "        else:\n",
    "            color = 'r'  # Outlier\n",
    "        axes[frame].scatter(pts1[i, 0], pts1[i, 1], s=6, c=color)\n",
    "        axes[frame].scatter(pts2[i, 0] + img1.shape[1], pts2[i, 1], s=6, c=color)  # Offset x for second image\n",
    "        axes[frame].set_title(f\"Frame {frame+1} and {frame+2}\")\n",
    "\n",
    "    break  # only first pair for demo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
