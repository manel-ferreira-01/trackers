{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import mediapy\n",
    "from scipy.io import savemat\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from aux.tracking_viz import generate_frame_image_cv\n",
    "from math.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# add tapnext to pythonpath\n",
    "import sys\n",
    "sys.path.append(\"/home/manuelf/tapnet\")\n",
    "from tapnet.tapnext.tapnext_torch import TAPNext\n",
    "from tapnet.tapnext.tapnext_torch_utils import restore_model_from_jax_checkpoint\n",
    "\n",
    "def init_tapnext(device):\n",
    "\n",
    "  tapnext = TAPNext(image_size=(256, 256)).to(device)\n",
    "\n",
    "  #set model to eval, not backprop\n",
    "  tapnext.eval()\n",
    "  for p in tapnext.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "  tapnext = restore_model_from_jax_checkpoint(tapnext, \"/home/manuelf/tapnet/tapnet/tapnext/tapnet/checkpoints/bootstapnext_ckpt.npz\")\n",
    "\n",
    "  return tapnext\n",
    "\n",
    "model_video_size = (256, 256)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "video = mediapy.read_video(\"../mast3r/images_in/apple.mp4\")\n",
    "video = ((video / 255) - 0.5) * 2 # normalize to [-1, 1]\n",
    "video_resized = mediapy.resize_video(video, model_video_size).astype(np.float32)\n",
    "\n",
    "tapnext = init_tapnext(device)\n",
    "\n",
    "# Design the query points array in the original video size\n",
    "step = 20\n",
    "ys, xs = np.meshgrid(np.linspace(8, video[0].shape[0]-step, step), np.linspace(8, video[0].shape[1]-step, step))\n",
    "query_points_initial = torch.tensor(\n",
    "    np.stack(\n",
    "    [np.zeros(len(xs.flatten())), xs.flatten(), ys.flatten()], axis=1\n",
    ")[None],dtype=torch.float32).to(device)\n",
    "\n",
    "#resize the querys points to the model size\n",
    "query_points_initial[0, :, 1:3] /= torch.tensor([video[0].shape[1] / model_video_size[0], video[0].shape[0] / model_video_size[1]]).to(device)\n",
    "\n",
    "#creata tensors for the video information\n",
    "video_tensor_original = torch.cat([torch.tensor(frame).unsqueeze(0) for frame in video]).unsqueeze(0).to(device) # 1, time, H, W, 3\n",
    "video_tensor_resized = torch.cat([torch.tensor(frame).unsqueeze(0) for frame in video_resized]).unsqueeze(0).to(device) # 1, time, H, W, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 37/49 [00:49<00:18,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new tracks: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:03<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# INFERENCE\n",
    "next_track_id = 0  # unique ID for every track\n",
    "track_histories = {}  # track_id -> list of (frame_index, (x, y)) or None\n",
    "active_tracks = {}  # current_idx -> track_id (used during current step)\n",
    "\n",
    "with torch.no_grad():\n",
    "  with torch.amp.autocast('cuda', dtype=torch.float32, enabled=True):\n",
    "    \n",
    "    tracks, tracks_logits, visible_logits, tracking_state = tapnext(video=video_tensor_resized[:, :1], query_points=query_points_initial)\n",
    "\n",
    "    num_feats = tracks.shape[2]\n",
    "    for i in range(num_feats):\n",
    "        track_id = next_track_id\n",
    "        next_track_id += 1\n",
    "        active_tracks[i] = track_id\n",
    "        track_histories[track_id] = [(0, tracks[0, 0, i, :2].cpu())]  # frame 0 position\n",
    "\n",
    "    for k in tqdm(range(1, video_tensor_resized.shape[1])):\n",
    "\n",
    "        tracks_step, tracks_logits_step, visible_logits_step, tracking_state = tapnext(\n",
    "            video=video_tensor_resized[:, k:k + 1],\n",
    "            state=tracking_state)\n",
    "        \n",
    "        # trakcs_step sai sempre bem \n",
    "\n",
    "        visible = (visible_logits_step.squeeze() > 0).cpu()\n",
    "        for i, track_tensor in enumerate(tracks_step[0, 0]):\n",
    "            track_id = active_tracks.get(i, None)\n",
    "            if track_id is not None:\n",
    "                if visible[i]:\n",
    "                    track_histories[track_id].append((k, track_tensor[:2].cpu()))\n",
    "                else:\n",
    "                    track_histories[track_id].append((k, None))  # Not visible\n",
    "\n",
    "        # New feature detection\n",
    "        new_tracks = add_new_tracks(tracks_step, query_points_initial)\n",
    "        \n",
    "        #if 0:\n",
    "        if new_tracks is not None and new_tracks.shape[1] > 30:\n",
    "            print(f\"Adding new tracks: {new_tracks.shape[1]}\")\n",
    "\n",
    "            new_tracks[0, :, 0] = 0  # set time to 0 for tapnext reinit\n",
    "\n",
    "            # retain visible active tracks from previous step\n",
    "            retained_indices = [i for i, v in enumerate(visible) if v]\n",
    "            retained_ids = [active_tracks[i] for i in retained_indices]\n",
    "\n",
    "            retained_tracks = tracks_step[0, 0, retained_indices].unsqueeze(0)\n",
    "            zero_time = torch.zeros((1, len(retained_indices), 1)).to(retained_tracks.device)\n",
    "            retained_tracks = torch.cat([zero_time, retained_tracks], dim=2)\n",
    "\n",
    "            concat_tracks = torch.cat([retained_tracks, new_tracks.cuda()], dim=1)\n",
    "\n",
    "            tracks_step, tracks_logits_step, visible_logits_step, tracking_state = tapnext(\n",
    "                video=video_tensor_resized[:, k].unsqueeze(0),\n",
    "                query_points=concat_tracks)\n",
    "\n",
    "            # update active_tracks dict\n",
    "            active_tracks = {}\n",
    "            for i, id in enumerate(retained_ids):\n",
    "                active_tracks[i] = id  # preserve old IDs\n",
    "\n",
    "            new_start = len(retained_ids)\n",
    "            for i in range(new_tracks.shape[1]):\n",
    "                track_id = next_track_id\n",
    "                next_track_id += 1\n",
    "                active_tracks[new_start + i] = track_id\n",
    "                track_histories[track_id] = [(k, tracks_step[0, 0, new_start + i, :2].cpu())]\n",
    "\n",
    "\n",
    "\n",
    "num_frames = video_tensor_resized.shape[1]\n",
    "output = {}\n",
    "\n",
    "for track_id, trajectory in track_histories.items():\n",
    "    coords = []\n",
    "    for t in range(num_frames):\n",
    "        step = next((pos for (frame, pos) in trajectory if frame == t), None)\n",
    "        coords.append(step if step is not None else torch.tensor([float('nan'), float('nan')]))\n",
    "    output[track_id] = torch.stack(coords)  # shape: [num_frames, 2]\n",
    "\n",
    "\n",
    "output_list = [trajectory.unsqueeze(0) for _, trajectory in output.items()]\n",
    "output_tensor = torch.cat(output_list, dim=0).unsqueeze(0).permute(0,2,1,3)  # shape: [1 , num_frames, num_feats, 2]\n",
    "\n",
    "output_tensor = output_tensor[:,:,:,[1,0]]\n",
    "output_tensor[:, : ,:,  :] *= torch.tensor([video[0].shape[1] / model_video_size[0], video[0].shape[0] / model_video_size[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:19<00:00,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_frame_image_cv(video_tensor_original.cpu(), output_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trackers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
